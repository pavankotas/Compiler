import inspect
import time

from Parser import AST
import re
import requests
import os, sys
import glob
from distutils.sysconfig import get_python_lib
import uuid
from keras.models import load_model
from jinja2 import Environment, FileSystemLoader
from FileUploader import ModelUploader

class Experiment:
    experiment_metadata = {}
    uploader = ModelUploader()

    def __init__(self, project_title, user):
        self.project_title = project_title
        self.user = user
        self.experiment_metadata = dict()
        # self.

    #For tracking experiment
    def track(self):
        self.experiment_metadata['project_name'] = self.project_title
        self.project_title += '_' + time.strftime("%m%d%y-%H%M%S") + '_'
        self.experiment_metadata['project_id'] = self.project_title + str(uuid.uuid4())
        self.experiment_metadata['experiment_id'] = self.experiment_metadata['project_id']
        self.experiment_metadata['user_name'] = self.user

        frame = inspect.stack()[1]
        module = inspect.getmodule(frame[0])
        self.extract_hyperparameters(module.__file__, module)

    #Retrieving parameters from
    def extract_hyperparameters(self, filename, module):
        astObj = AST()
        hyperParams = astObj.ParseAst(filename, self.project_title)
        for i in range(0, len(hyperParams)):
            if 'epochs' in hyperParams[i] :
                self.experiment_metadata['epochs'] = hyperParams[i].split('=')[1]
            if 'batch_size' in hyperParams[i]:
                self.experiment_metadata['batch_size'] = hyperParams[i].split('=')[1]
            #Extract metadata from the model file.

        #Adding Callbacks and Checkpoint Code Dynamically

        flag = 1
        temp = module
        with open(filename) as myfile:
            if 'CSVLogger' in myfile.read():
                    return
            temp_file_location = get_python_lib()
            temp_file_location = temp_file_location.replace('\\', '/')
            with open(temp_file_location + '/temp.py', 'w') as file:
                source_lines = inspect.getsourcelines(temp)
                for code in source_lines:
                    if (isinstance(code, (list))):
                        for line in range(0, len(code)):
                            #Adding import functions and call backs
                            if 'import' in code[line] and flag == 1:
                                file.write(str('from keras.callbacks import CSVLogger, ModelCheckpoint\n'))
                                file.write(str(file.write(code[line])) + '\n')
                                flag = 0
                            elif '.fit(' in code[line]:
                                csv_file_path = temp_file_location + '/training.log'
                                file.write("csv_logger = CSVLogger('"+csv_file_path+"')" + '\n')
                                weights_file_path = temp_file_location + '/weights-improvement-{epoch:02d}-{acc:.2f}.hdf5'
                                file.write(str('filepath="'+weights_file_path+'"') + '\n')
                                file.write(str("checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=False, mode='max')" + '\n'))
                                file.write(str("callbacks_list = checkpoint" + '\n'))
                                code[line] = code[line].replace(')', ',callbacks=[csv_logger, callbacks_list])')
                                file.write(code[line])
                            else:
                                file.write(code[line])

        #Executing the temp file
        os.system(sys.executable + ' ' + temp_file_location+'/temp.py')

        model_file = glob.glob(temp_file_location + '/*.hdf5')
        epochs = int(self.experiment_metadata['epochs'])

        #Loading model and parameters
        model = load_model(model_file[epochs-1])
        self.experiment_metadata['input_shape'] = model.input_shape
        self.experiment_metadata['layers_count'] = len(model.layers)
        self.experiment_metadata['output_shape'] = model.output_shape
        self.experiment_metadata['Optimizer'] = model.optimizer.__class__.__name__
        self.experiment_metadata['LossFunction'] = model.loss

        #Model and Log file
        self.experiment_metadata['callbacks_log'] = temp_file_location + '/training.log'
        self.experiment_metadata['model_file'] = model_file[epochs - 1]

        #Generate Predict Function
        # output = self.generate_predict(model)
        # with open(temp_file_location + '/auto_predict.py', 'w') as file:
        #     file.write(output)

        self.experiment_metadata['predict_function'] = temp_file_location + '/auto_predict.py'
        print(self.experiment_metadata)


        self.saveToLocalDB(self.experiment_metadata)
        #Python_file, Metadata_dict(), project name
        #self.saveToDB(filename, self.experiment_metadata, self.project_title)

        #Deleting all the files that are generated by ModelKB
        os.remove(temp_file_location+ '/temp.py')
        os.remove(temp_file_location+'/training.log')
        os.remove(temp_file_location+'/auto_predict.py')
        for i in range(0, len(model_file)):
            os.remove(model_file[i])
        sys.exit(0)

    def saveToLocalDB(self,expData):
        self.uploadFiles(expData)
        files = ['callbacks_log', 'model_file', 'predict_function']
        data = {}
        for key in expData.keys():
            if key not in files:
                data[key] = expData[key]

        data['fileIDs'] = Experiment.uploader.filesArray
        response = requests.post('http://localhost:4000/kerasfitparameters', json=data)
        print("checkin successfull")

    def uploadFiles(self,expData):
        files = ['callbacks_log', 'model_file', 'predict_function']
        for key in expData.keys():
            if key in files:
                res = Experiment.uploader.UploadFile(expData[key])
                if 'file_uploaded' in res.keys():
                    Experiment.uploader.filesArray.append(res['file_id'])

    def generate_predict(self, model):
        # Template Files
        file_loader = FileSystemLoader("templates")
        env = Environment(loader=file_loader)
        template = env.get_template('predict_template.txt')

        # Dictionary that includes the parameters to pass through the tempalte
        inference_data = {}
        inference_data['input_shape'] = model.input_shape[1:]

        if (inference_data['input_shape'][0] == 3):
            inference_data['color_image'] = True
        else:
            inference_data['color_image'] = False
        inference_data['data_augumentation'] = True

        output = template.render(inference_data=inference_data)

        return output


    #Saving information to database
    # def saveToDB(self, ls, projName):
    #     fitParams = '(x|y|batch_size|epochs|verbose|callbacks|validation_split|validation_data|shuffle|class_weight|' \
    #                 'sample_weight|initial_epoch|steps_per_epoch|validation_steps|validation_steps)'
    #     data = {}
    #     for ob in ls:
    #         ob = ob.split('=')
    #         if re.search(fitParams, ob[0].strip()):
    #             data[ob[0].strip()] = ob[1].strip()
    #     data['experimentID'] = '1000'
    #     response = requests.post('http://localhost:4000/kerasfitparameters', json=data)












